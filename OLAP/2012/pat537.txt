A pose for an object in a scene is determined by first rendering sets of virtual images of a model of the object using a virtual camera. Each set of virtual images is for a different known pose the model, and constructing virtual depth edge map from each virtual image, which are stored in a database. A set of real images of the object at an unknown pose are acquired by a real camera, and constructing real depth edge map for each real image. The real depth edge maps are compared with the virtual depth edge maps using a cost function to determine the known pose that best matches the unknown pose, wherein the matching is based on locations and orientations of pixels in the depth edge maps.
Claims We claim: 1. A method for determining a pose of an object in a scene, comprising a processor for performing steps of the method, comprising the steps of: rendering sets of virtual images of a model of the object using a virtual camera, wherein each set of virtual images is for a different known pose of the model, and wherein the model is illuminated by a set of virtual light sources, and wherein there is one virtual image for each virtual light source in a particular set for a particular know pose; constructing virtual depth edge map from each virtual image; storing each set of depth edge maps in a database and associating each set of depth edge maps with the corresponding known pose; acquiring a set of real images of the object in the scene with a real camera, wherein the object has an unknown pose, and wherein the object is illuminated by a set of real light sources, and wherein there is one real image for each real light source; constructing real depth edge map for each real image; and matching the real depth edge maps with the virtual depth edge maps of each set of virtual images using a cost function to determine the known pose that best matches the unknown pose, wherein the matching is based on locations and orientations of pixels in the depth edge maps. 2. The method of claim 1, where the real camera and the virtual camera are conventional, and the edges of the real and virtual images are used for pose estimation. 3. The method of claim 2, wherein the method is used for object detection and localization in an image from a database of stored query edge templates for various objects. 4. The method of claim 2, wherein the edges from the real and virtual images are partitioned into discrete orientation channels, and wherein the cost function sums matching scores across the orientation channels. 5. The method of claim 4, wherein the cost function is .function..times..di-elect cons..times..times..di-elect cons..times..lamda..times..PHI..function..PHI..function. ##EQU00014## wherein U={u.sub.i} are virtual pixels in the virtual edge maps, V={v.sub.j} are real pixels in the real image edge maps, .phi. is an orientation of each pixel, .lamda. is a weighting factor, and n=|U|. 6. The method of claim 5, wherein the directions .phi. are computed modulo .pi., and an orientation error gives a minimum circular difference between the two directions. 7. The method of claim 1, wherein the camera is arranged on a robot arm for manipulating the object. 8. The method of claim 1, wherein the model is a computer-aided design model. 9. The method of claim 1, wherein the model is a set of edges for possible poses of the object. 10. The method of claim 1 where multiple models for different objects are stored simultaneously. 11. The method of claim 1, further comprising; acquiring an ambient image of the scene using ambient light; subtracting the ambient image from each real image. 12. The method of claim 1, wherein the matching uses directional chamfer matching to determine a coarse pose; and an optional procedure for refining the coarse pose. 13. The method of claim 1, further comprising: dividing each real image by a maximum intensity image to determine a ratio image, and wherein the matching is based on the ratio images. 14. The method of claim 1, further comprising: quantizing each virtual image and each real image into discrete orientation channels, and wherein the cost function sums matching scores across the orientation channels. 15. The method of claim 1, further comprising: representing pixels in the virtual image and the real images by line segments; and aligning the lines segments of the virtual images and the real images. 16. The method of claim 1, further comprising: representing pixels in the virtual image and the real images by line segments; and aligning the lines segments of the virtual images and the real images. 17. The method of claim 5 or 16, wherein the cost function for a given location is computed in sub-linear time in the number of edge points using 3D distance transforms and directional integral images. 18. The method of claim 1, wherein the edges can be computed using a conventional camera and Canny edge detection. 19. The method of claim 1, wherein a gallery of hand drawn or exemplar objects are detected and localized in images using the cost function and fast matching algorithm. 20. The method of claim 19, wherein poses of rigid or deformable objects are estimated using a gallery of exemplar images or shapes. 21. The method of claim 1 as applied to human body pose estimation. 22. The method of claim 1 as applied to object detection and localization in images. 