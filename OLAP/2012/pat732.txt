In a multi-participant modeled virtual reality environment, avatars are modeled beings that include moveable eyes creating the impression of an apparent gaze direction. Control of eye movement may be performed autonomously using software to select and prioritize targets in a visual field. Sequence and duration of apparent gaze may then be controlled using automatically determined priorities. Optionally, user preferences for object characteristics may be factored into determining priority of apparent gaze. Resulting modeled avatars are rendered on client displays to provide more lifelike and interesting avatar depictions with shifting gaze directions.
Claims What is claimed is: 1. A method for animating an avatar's gaze, the method comprising: providing a digital representation of an avatar and a modeled scene in a computer memory, wherein the digital representation includes at least one modeled eye; determining a field of view for the modeled eye, wherein the field of view encompasses visual targets in the modeled scene; directing the modeled eye to gaze at different selected visual targets in the field of view in an automated sequence of changing gaze directions wherein duration of gaze directed at each of the selected visual targets is determined at least in part using an attractiveness value for each of the visual targets based at least in part on predefined user preferences for specified characteristics of prospective visual targets and a geometrical relationship between each visual target and the modeled eye; and outputting data configured to cause a client computer to display a rendered view of the modeled scene and avatar. 2. The method of claim 1, further comprising assigning an attractiveness value to each visual target in the field of view. 3. The method of claim 2, further comprising the step of determining the attractiveness value based on a measure of movement of the visual target within the field of view. 4. The method of claim 2, further comprising the step of determining the attractiveness value based on a measure of sound associated with the visual target. 5. The method of claim 1, further comprising determining the sequence of gazing for the modeled eye further based on the predefined user preference data applied to the visual targets. 6. The method of claim 1, wherein the predefined user preferences comprise a preference value for any one or more of the specified characteristics selected from the group consisting of gender, sexual orientation, race, hair color, eye color, body type, and clothes. 7. The method of claim 1, wherein the attractiveness value for each visual target is fixed. 8. The method of claim 1, wherein the attractiveness value for each visual target varies based on changing attributes of each visual target during modeling of the scene. 9. The method of claim 8, wherein the attractiveness value is determined based on a location of the visual targets in relation to the avatar. 10. The method of claim 1, wherein the avatar comprises a second modeled eye and wherein modeled movement of the second modeled eye tracks modeled movement of the modeled eye. 11. The method of claim 1, wherein the field of view is determined based on an orientation and forward facing direction of the modeled eye in the scene. 12. The method of claim 1, further comprising determining the attractiveness value of the selected visual targets based at least in part on weighting features of the visual targets according to the predefined user preferences. 13. The method of claim 1, further comprising determining a viewpoint and gaze direction corresponding to the orientation of the modeled eye in the scene. 14. The method of claim 13, further comprising outputting the viewpoint and gaze direction to a client associated with the avatar, configured for rendering the scene from the viewpoint and gaze direction. 15. A non-transitory computer-readable storage medium encoded with instructions operative to cause a computer to perform the steps of: modeling an avatar, an eye belonging to the avatar, and a scene in a computer memory; determining a field of view for the modeled eye, wherein the field of view encompasses target objects in the modeled scene; directing the modeled eye to change orientations as if gazing at different selected target objects in the field of view in an automated sequence of changing gaze directions wherein duration of gaze directed at each of the selected target objects is determined using a defined attractiveness characteristic for each of the target objects based at least in part on predefined user preferences for specified characteristics of prospective target objects and a distance between each target object and the modeled eye; and outputting data configured to enable a client computer to display a rendered view of the modeled scene, eye and avatar. 16. The non-transitory computer-readable storage medium of claim 15, further comprising encoded instructions for defining an attractiveness characteristic to each target object in the field of view. 17. The non-transitory computer-readable storage medium of claim 16, further comprising encoded instructions for defining the attractiveness characteristic based on an amount of movement of the target object within the field of view. 18. The non-transitory computer-readable storage medium of claim 16, further comprising encoded instructions for defining the attractiveness characteristic based on an amount of sound associated with the target object. 19. The non-transitory computer-readable storage medium of claim 15, further comprising encoded instructions for determining the sequence of eye orientations further based on the predefined user preferences. 20. The non-transitory computer-readable storage medium of claim 15, further comprising instructions for determining the field of view based on an orientation of the modeled eye in the scene. 21. The non-transitory computer-readable storage medium of claim 15, further comprising instructions for determining the attractiveness value of the selected target objects based at least in part on weighting features of the target objects according to the predefined user preferences. 22. The non-transitory computer-readable storage medium of claim 15, further comprising instructions for determining a viewpoint and gaze direction corresponding to an orientation of the modeled eye in the scene. 23. The non-transitory computer-readable storage medium of claim 21, further comprising instructions for outputting the viewpoint and gaze direction to a client associated with the avatar configured for rendering the scene from the viewpoint and gaze direction. 