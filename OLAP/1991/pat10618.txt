An apparatus and method for picking up and manipulating randomly oriented and randomly positioned objects moving on an object belt and transferring them to randomly oriented and randomly positioned destinations moving on a destination belt. An image processing unit using a vision system identifies and locates objects and destinations in successive overlapping vision windows up to a predetermined optimum number of objects. The locations of those objects and destinations are entered in an output queue which is transmitted to the object and destination location queues of a first robot motion controller. The first robot picks up and deposits to destinations all the objects it can in the time available while the objects and destinations pass, and enters the locations of the objects not picked up and destinations to which no object is placed in an output queue which is transmitted to the object and destination location queues of a second robot motion controller. The robots can be arranged in a series of any number and the image processing units and vision systems can also be arranged in a series.
Claims What is claimed is: 1. A method for picking up objects from a moving object conveyor belt and transferring them to destinations on a moving destination conveyor belt, comprising: receiving a pixel image of a first object vision window image portion of said object belt containing images of said objects using first object image-receiving means; digitizing said first object vision window image by assigning a gray scale level and a coordinate location to the pixels in said first object vision window image; electronically determining and transmitting, utilizing first data transmission mean, the locations of at least one of said objects represented by object images in said first object vision window image to a first motion controller directing a first robot means associated with said first object image-receiving means located in the direction of belt travel with respect to said first object image-receiving means; receiving a pixel image of a first destination vision window image portion of said destination belt containing images of said destinations using first destination image-receiving means; digitizing said first destination vision window image by assigning a gray scale level and a coordinate location to the pixels in said first destination vision window image; electronically determining and transmitting, utilizing first data transmission means, the locations of at least one of said destinations represented by destination images in said first destination vision window image to said first motion controller; picking up at least one of said objects represented by object images in said first object vision window, image, the locations of which were transmitted to said first motion controller, using said first robot means; placing said at least one of said objects to at least one of said destinations represented by destination images in said first destination vision window, the locations of which were transmitted to said first motion controller, using said first robot means; transmitting, utilizing second data transmission means, the locations of at least one of the objects represented by object images in said first object vision window image which were not picked up by said first robot means, from said first motion controller to a second motion controller directing a second robot means associated with said first object image-receiving means located in the direction of belt travel with respect to said first robot means; transmitting, utilizing said second data transmission means, the locations of at least one of the destinations represented by destination images in said first destination vision window image to which no object was placed by said first robot means, from said first motion controller to said second motion controller; picking up at least one of said objects represented by object images in said first object vision window image, the locations of which were transmitted to said second motion controller, using said second robot means; and placing said at least one of said objects represented by object images in said first object vision window image, the locations of which were transmitted to said second motion controller, to at least one of said destinations represented by destination images in said first destination vision window, the locations of which were transmitted to said second motion controller, using said second robot means. 2. The method of claim 1, further comprising: establishing a binary image of said digitized first object vision window image and first destination vision window image by assigning 0 or 1 to each pixel gray scale value depending on whether said gray scale value is above or below a predetermined threshold value; and determining the location and configuration of said objects and destinations represented by images in said first object vision window image and first destination vision window image using said binary image. 3. The method of claim 1, further comprising: establishing a pick-up window area on said object belt for each said robot means associated with said first object image-receiving means, and wherein each motion controller which directs each robot means associated with said first object image-receiving means repeatedly directs said robot means to pick up certain of the objects in said robot means' pick-up window area; and establishing a placement window area on said destination belt for each said robot means associated with said first destination image receiving means, and wherein each motion controller which directs each robot means associated with said first destination image-receiving means repeatedly directs said robot means to deposit said certain of said objects in said robot means' placement window area. 4. The method of claim 3, wherein each object that each motion controller directs each robot means to pick up is the object in said robot means' pick-up window area that is farthest in the direction of belt travel; and each destination to which each motion controller directs each robot to place objects is the destination in said robot means' placement window area that is farthest in the direction of belt travel. 5. The method of claim 1, wherein the locations of objects that are smaller than a predetermined minimum area are not transmitted to said first motion controller. 6. The method of claim 1, wherein the locations of objects and destinations with images that are larger than a predetermined maximum area are determined by comparing said larger object and destination images to a predetermined prototype image, and the locations of objects and destinations represented by those larger object and destination images that do not match any of the prototype image within preset tolerances are not transmitted to said first motion controller. 7. The method of claim 5, wherein object and destination images larger than a predetermined maximum area are compared to a prototype image, and those portions of said larger images that match said prototype image within preset tolerances are deemed to be images of individual objects and destinations and their locations are transmitted to said first motion controller. 8. The method of claim 7, further comprising establishing an object vision window image boundary at the edge of said object vision window image and a destination vision window image boundary at the edge of said destination vision window image, and wherein the objects and destinations that are at least partially in said vision window image boundary are not picked up. 9. The method of claim 1, wherein the locations of objects in excess of a predetermined optimum number in each object vision window image are not transmitted to said first motion controller. 10. The method of claim 9, wherein said objects and destinations are located and counted in the order of their distance from one edge of said vision window images. 11. The method of claim 9, wherein said optimum number is equal to the number of objects that the robots associated with said first image-receiving means can pick up considering variables including the speed of the belt and the character of the object. 12. The method of claim 1, further comprising: receiving a pixel image of a subsequent object vision window portion of said object belt containing images of said objects using said first object image-receiving means, said subsequent object vision window having an edge in the direction of belt travel that is a predetermined distance in the direction opposite belt travel from the edge in the direction of belt travel of the adjacent object vision window in the direction of belt travel; digitizing said subsequent object vision window image by assigning a gray scale level and a coordinate location to the pixels in said subsequent object vision window image; electronically determining and transmitting, utilizing said first data transmission means, the locations of at least one of said objects represented by object images in said subsequent object vision window image to said first motion controller directing said first robot means; receiving a pixel image of a subsequent destination vision window image portion of said destination belt containing images of said destinations using first destination image-receiving means, said subsequent destination vision window having on edge in the direction of belt travel that is a predetermined distance in the direction opposite belt travel from the edge in the direction of belt travel of the adjacent destination vision window in the direction of belt travel; digitizing said subsequent destination vision window image by assigning a gray scale level and a coordinate location to the pixels in said subsequent destination vision window image; electronically determining and transmitting, utilizing first data transmission means, the locations of at least one of said destinations represented by destination images in said subsequent destination vision window image to said first motion controller directing a first robot means associated with said first image-receiving means located in the direction of belt travel with respect to said first image-receiving means; picking up at least one of said objects represented by object images in said subsequent object vision window image the locations of which were transmitted to said first motion controller, using said first robot means; placing said at least one of said objects to at least one of said destinations represented by destination images in said subsequent destination vision window, the locations of which were transmitted to said first motion controller using said first robot means; transmitting, utilizing said second data transmission means, the locations of at least one of the objects represented by object images in said subsequent object vision window image which were not picked up by said first robot means, from said first motion controller to said second motion controller; picking up at least one of said objects represented by object images in said subsequent object vision window image, the locations of which were transmitted to said second motion controller, using said second robot means; and placing said at least one of said objects represented by object images in said subsequent object vision window image, the locations of which were transmitted to said motion controller, to at least one of said destinations represented by destination images in said subsequent destination vision window, the locations of which were transmitted to said second motion controller, using said second robot means. 13. The method of claim 12, wherein each vision window overlaps each adjacent vision window, and wherein the location of objects and destinations whose locations were in the adjacent vision window in the direction of belt travel and were previously transmitted to said first motion controller, are not transmitted again to said first motion controller. 14. The method of claim 13, wherein each robot means pick-up window area and destination placement window area are areas from which said robot means can efficiently pick up objects without excess motion of the robot means. 15. The method of claim 14, wherein the area of said robot means pick-up window is larger for robot means away from said first object image-receiving means with which they are associated than for robot means close to said first object image-receiving means with which they are associated. 16. The method of claim 3, wherein said robot means includes a plurality of pick-up cups, and each cup is filled with an object before all the objects in the cups are moved to the destinations. 17. The method of claim 13, further comprising: receiving a pixel image of another first object vision window portion of said object belt using second object image-receiving means in the direction of belt travel from the last robot means in the direction of belt travel associated with said first object image-receiving means; digitizing said another first object vision window image by assigning a gray scale value level and coordinate location to pixels is said another first object vision window image; electronically determining and transmitting, utilizing third data transmission means, the locations of at least one of said objects to another first motion controller directing another first robot means located in the direction of belt travel with respect to said second object image-receiving means; receiving a pixel image of another first destination vision window image portion of said destination belt containing images of said destinations using second destination image-receiving means; digitizing said another first destination vision window image by assigning a gray scale level and a coordinate location to the pixels in said another first destination vision window image; electronically determining and transmitting, utilizing said third data transmission means, the locations of at least one of said destinations to another first motion controller directing another first robot means located in the direction of belt travel with respect to said second destination image-receiving means; picking up at least one of said objects represented by object images in said another first vision window image, the locations of which were transmitted to said another first motion controller, using said another first robot means; placing said at least one of said objects represented by object images in said another first object vision window image, the locations of which were transmitted to said another first motion controller, to at least one of said destinations represented by destination images in said another first destination vision window, the locations of which were transmitted to said another first motion controller, using said another first robot means; and transmitting, utilizing fourth data transmission means, the locations of at least one of the objects represented by object images in said another first vision window image which were not picked up by said another first robot means, from said another first motion controller to another second motion controller directing another second robot means associated with said second object image-receiving means located in the direction of belt travel with respect to said another first robot means; picking up at least one of said objects represented by object images in said another first vision window image, the locations of which were transmitted to said another second motion controller, using said another second robot means; and transmitting, utilizing said fourth data transmission means, the locations of at least one of the destinations represented by destination images in said another first destination vision window image to which no object was placed by said another first robot means, from said another first motion controller to said another second motion controller directing another second robot means associated with said second destination image-receiving means located in the direction of belt travel with respect to said another first robot means; placing said at least one of said objects represented by object images in said another first object vision window image, the locations of which were transmitted to said another second motion controller, to at least one of said destinations represented by destination images in said another first destination vision window, the locations of which were transmitted to said another second motion controller, using said another second robot means. 18. A method for picking up objects from a moving object conveyor belt and moving them to a moving destination conveyor belt, comprising: receiving a series of overlapping images of object vision window portions of said object belt and destination vision window portions of said destination belt using image-receiving means, said object vision windows and destination vision windows being fixed on the belt and moving therewith; digitizing each of the object vision window images and destination vision window images in said series, said digitizing including assigning a coordinate location to pixels in said image; electronically determining and transmitting, utilizing first data transmission means, the locations of at least one of said objects and at least one of said destinations to a first motion controller directing a first robot means, said determining and transmitting being done a vision window image at a time for each vision window image in said series; picking up at least one of said objects from said object belt using said first robot means at the time it moves within the reach of said first robot means; placing said at least one of said objects to said at least one of said destinations; transmitting, utilizing second data transmission means, the locations of at least one of the objects which were not picked up and the destinations to which no object was placed by said first robot means, from said first motion controller to a second motion controller directing a second robot means located in the direction of belt travel with respect to said first robot means; picking up at least one of said objects using said second robot means at the time the object moves within the reach of said second robot means; and placing said at least one of said objects to said at least one of said destinations. 19. A system for picking up objects located on a moving object conveyor belt and transferring them to destinations on a moving destination conveyor belt, comprising: image-receiving means to receive pixel images of a series of vision window portions of each of said belts; image processing means electronically connected with said image-receiving means for assigning a location to objects and destinations represented in said images; a plurality of robot means in series adjacent to said belts, each with a directing motion controller, wherein the motion controller directing the first robot means receives the locations of objects and destinations from said image processing means utilizing first data transmission means and directs the first robot means to pick up certain of said objects and place them at certain of said destinations, and wherein the motion controller for each robot in said series except the last transmits, utilizing subsequent data transmission means to the next motion controller, the location of objects not picked up and destinations to which no object was placed by the robot it directs or any preceding robot. 20. The system of claim 19, wherein said image processing means determines and transmits to the first motion controller the locations of no more than a predetermined number of objects in each vision window in said series, said number being based on variables including the belt speed and number of robot means in said robot means series. 21. The system of claim 20, wherein said vision windows overlap in the direction of belt travel by a predetermined distance. 22. The system of claim 21 wherein the locations of objects and destinations in each vision window is determined and transmitted to the first motion controller a vision window at a time, and the locations of objects and destinations transmitted to the first motion controller for one vision window are not transmitted to the first motion controller for an adjacent overlapping vision window. 23. The system of claim 22, wherein the objects and destinations that can be reached by each robot means are picked up by said robot means in the order of the direction of belt movement. 24. The system of claim 23, wherein the image processing means does not transmit to the first motion controller the locations of objects smaller than a predetermined minimum area. 25. The system of claim 24, wherein the image processing means compares the shape and size of images that are larger than a predetermined maximum area with a prototype image and assigns separate object and destination locations to each object and destination represented by said larger images if any portion of the larger image matches the prototype image within a predetermined tolerance. 26. The system of claim 24, wherein each of said robot means includes a plurality of end effector cups for picking up objects. 27. A system for picking up stationary objects and transferring them to destinations on a moving destination conveyor belt, comprising: image-receiving means to receive pixel images of a series of vision window portions of said destination belt; image processing means electronically connected with said image-receiving means for assigning a location to destinations represented in said images; a plurality of robot means in series adjacent to said destination belt, each with a directing motion controller, wherein the motion controller directing the first robot means receives the locations of destinations from said image processing means utilizing first data transmission means and directs the first robot means to pick up objects and place them at certain of said destinations, and wherein the motion controller for each robot in said series except the last transmits, utilizing subsequent data transmission means to the next motion controller, the location of destinations to which no object was placed by the robot it directs or any preceding robot. 28. The system of claim 27, wherein said vision windows overlap in the direction of belt travel by a predetermined distance. 29. The system of claim 27 wherein the locations of destinations and destinations in each vision window is determined and transmitted to the first motion controller a vision window at a time, and the locations of destinations transmitted to the first motion controller for one vision window are not transmitted to the first motion controller for an adjacent overlapping vision window. 30. The system of claim 29, wherein the destinations that can be reached by each robot means are picked up by said robot means in the order of the direction of destination belt movement. 31. The system of claim 30, wherein the image processing means compares the shape and size of images that are larger than a predetermined maximum area with a prototype image and assigns separate and destination locations to each destination represented by said larger images if any portion of the larger image matches the prototype image within a predetermined tolerance. 32. The system of claim 31, wherein each of said robot means includes a plurality of end effector cups for picking up objects. 