The apparatus and method measure the three-dimensional surface shape of a surface without contact with the surface, and without any physical constraint on the device during measurement. The device is a range-sensor or scanner, and in one embodiment is a laser-camera sensor, which has a portable camera and multi-line light projector encased in a compact sensor head, and a computer. The apparatus provides three-dimensional coordinates in a single reference frame of points on the surface. The sensor head does not have to be physically attached to any mechanical positioning device such as a mechanical arm, rail, or translation or rotation stage, and its position in three-dimensional space does not have to be measured by any position-tracking sensor. This allows unrestricted motion of the sensor head during scanning, and therefore provides much greater access to surfaces which are immovable, or which have large dimensions or complex shape, and which are in confined spaces such as interior surfaces. It also permits measurement of a surface to be made by a continuous sweeping motion rather than in stages, and thus greatly simplifies the process of measurement. The apparatus can be hand-held, mounted on any moving device whose motion is unknown or not accurately known, or airborne. The apparatus and method also permit unknown and unmeasured movement of the object whose surface is to be measured, which may be simultaneous with the movement of the range-sensor head.
Claims What is claimed is: 1. A method of obtaining three-dimensional data for a surface, said method comprising the steps of: a. obtaining three-dimensional information of various points of the surface in a plurality of at least partially overlapping views by a sensor head, where the positions and relative positions of said surface and said sensor head are not known and are derived from said information of said surface obtained by the sensor head; b. registering a plurality of said views, by computing the transformations between overlapping views; and c. integrating said views into a common reference frame by applying appropriate transformations to points of each registered view. 2. A method as recited in claim 1, wherein said three-dimensional information is in the form of points along several spaced-apart surface profiles. 3. A method as recited in claim 2, wherein said profiles are obtained by using images from a camera positioned to view the intersection of the light of several generally parallel planes of light intersecting with a surface by a projector at an angle from the camera, for a plurality of overlapping views of the surface. 4. A method as recited in claim 3, wherein said three-dimensional information of points along said profiles is obtained by processing two-dimensional data from said images from said camera, to reconstruct three-dimensional coordinates in the local frame of a sensor head comprising said camera and said projector, of a plurality of points which comprise the profiles. 5. A method as recited in claim 4, wherein said processing to reconstruct three-dimensional coordinates is based on previously-obtained calibration mathematical function for said sensor head, said calibration mathematical function having been obtained from calibration via execution of a first algorithm to determine a mathematical mapping relationship between any two-dimensional image point and its corresponding three-dimensional coordinate which represents a point of a profile. 6. A method as recited in claim 4, wherein said processing of two-dimensional information is carried out by execution of a second algorithm on a computer connected to receive said images from said camera, wherein said second algorithm: a. detects the profiles in the two-dimensional images from the camera in real-time during a scan by said sensor head; b. extracts the coordinates from two edges of a profile; c. saves the two-dimensional image coordinates which correspond to edge points; d. in scanning across the camera image, counts the profiles being detected to identify and label each measured point with the proper profile; and e. stores information from the preceding steps of this claim. 7. A method as recited in claim 6, wherein said steps of extracting and saving coordinates includes smoothing raw profile data by a simple average of neighboring points or by spline or polynomial piecewise curve fitting, on the two-dimensional image profile, or on the three-dimensional curve once reconstructed. 8. A method as recited in claim 4, wherein said acquisition of two-dimensional images is carried out by execution of a second algorithm on a computer connected to receive said images from said camera, where said second algorithm: a. detects the profiles in the two-dimensional images in real-time during the scan; b. extracts the coordinates from two edges of a profile,. by scanning across the camera image and detecting abrupt changes in pixel intensity; c. computes the two-dimensional image coordinates which correspond to the center of the profile and saves such information; d. in scanning across the camera image, counts the profiles being detected to identify and label each measured point with the proper profile; and e. stores the necessary useful information from the preceding steps. 9. A method as recited in claim 8, wherein said step of computing the center of the profile comprises averaging coordinates of left and right edges at each point along a profile to obtain the center of the profile. 10. A method as recited in claim 8, wherein said step of computing the center of the profile comprises computing the two-dimensional image coordinates which correspond to the peak light intensity across the width of the profile. 11. A method as recited in claim 8, wherein said steps of extracting and saving coordinates includes smoothing raw profile data by a simple average of neighboring points or by spline or polynomial piecewise curve fitting, on the two-dimensional image profile, or on the three-dimensional curve once reconstructed. 12. A method as recited in claim 5, wherein said processing to reconstruct three-dimensional coordinates includes reconstrucing three-dimensional coordinates using the mathematical mapping relationship. 13. A method as recited in claim 1, wherein said step of registering said views includes searching locally around the points to match points of said plurality of views. 14. A method as recited in claim 1, wherein said step of integrating said views includes applying view-pair transformations sequentially to bring the registered views into a single reference frame. 15. A method of obtaining three-dimensional data for a surface, said method comprising the steps of: a. obtaining three-dimensional data of various points of the surface in a plurality of at least patially overlapping views by use of a range-sensor head, where the positions and relative positions of the surface and the range-sensor head are not tracked or measured by any other sensor or device, and are otherwise unknown, the three-dimensional data being in the form of points along several spaced apart surface profiles; b. registering a plurality of said views, by computing the transformations between overlapping views; and c. integrating said views into a common reference frame by applying appropriate transformations to points of each registered view. 16. A method as recited in claim 15, wherein said profiles are obtained by using images from a camera positioned to view the intersection of the light of several generally parallel planes of light intersecting with said surface at an angle from the camera, for a plurality of overlapping views of the surface. 17. Apparatus for obtaining three-dimensional data of a surface, comprising: a sensor head comprising a light projector and a camera, said light projector projecting at least several generally parallel lines on the surface, said camera being positioned for capturing an image of said lines projected onto said surface at an angle from said light projector and said lines on said surface; and a computer for analyzing said image, wherein said computer is programmed to: a. compute three-dimensional data of various points of the surface in a plurality of at least partially overlapping views from the camera; b. register a plurality of said views, by computing the transformations between overlapping views; and c. integrate said views into a common reference frame by applying appropriate transformations to points of each registered view; wherein the positions and relative positions of said surface and said sensor head are not tracked or measured by any other sensor or device. 18. Apparatus as recited in claim 17, wherein said light projector comprises a plurality of light sources mounted to project a corresponding number of approximately parallel planes of light. 19. Apparatus as recited in claim 17, wherein said light projector comprises a single light source with a several-line optic lens mounted to project several approximately parallel planes of light. 20. Apparatus as recited in claim 17, wherein said light projector comprises a single light spot projected using beam-splitters, prisms and rotating mirrors, to define said generally parallel lines. 21. The method as recited in claim 13, further including surface fitting the points of each view. 