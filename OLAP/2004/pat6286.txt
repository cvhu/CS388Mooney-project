With a robot and a control method for it, information is read in from the outside, based on which a particular object is detected, and it is judged upon detecting the object whether or not the object satisfies given conditions, based on the judgment results of which a robot generates predetermined actions. Thus, a robot can be realized that acts naturally like a living thing. Consequently, its entertaining quality for human beings can be increased greatly.
Claims What is claimed is: 1. A robot apparatus having moveable parts, comprising: means for detecting and identifying a person from plurality of person profiles; means for storing an emotion value corresponding to the detected and identified person representative of an emotion towards the detected and identified person; means for changing the emotion value based on a behavior performed by said detected and identified person; means for storing an action model to decide an action of the robot apparatus, wherein the action model decides the action of the robot apparatus based upon the emotion values; and means for controlling the moveable parts of the apparatus in accordance with the action decided by the action model. 2. The robot apparatus of claim 1, wherein said means for detecting a person generates face pattern data based on an input image and specifies a particular person based on the face pattern data. 3. The robot apparatus of claim 1, wherein said emotion value information comprises information on whether to have a favorable impression or a disfavorable impression toward said particular person. 4. The robot apparatus of claim 3, wherein said emotion value information further comprises information on an intensity of the favorable or unfavorable impression. 5. The robot apparatus of claim 1, wherein said emotion value is changed based upon whether an action performed by said person is perceived by said robot apparatus as being friendly or unfriendly. 6. A method for controlling a robot apparatus having moveable parts, comprising the steps of: detecting and identifying a person from a plurality of person profiles; storing an emotion value corresponding to the detected and identified person representative of an emotion towards a detected and identified person; changing the emotion value based on a behavior performed by said detected person; storing an action model to decide an action of the robot apparatus, wherein the action model decides the action of the robot apparatus based upon the emotion values; and controlling the moveable parts of the apparatus in accordance with the action decided by the action model. 7. The method of claim 6, further comprising the step of generating face pattern data based on an input image and specifying a particular person based on the face pattern data. 8. The method of claim 6, wherein said emotion value information comprises information on whether to have a favorable impression or a disfavorable impression toward said particular person. 9. The method of claim 8, wherein said emotion value information further comprises information on an intensity of the favorable or unfavorable impression. 10. The method of claim 6, further comprising the step of changing said emotion value based upon whether an action performed by said person is perceived by said robot apparatus as being friendly or unfriendly. 