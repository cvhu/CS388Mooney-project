A method is provided for determining a calibration relationship between a reference frame of motion of an object and a reference frame of a camera that generates images of the object. The method includes the steps of coupling a target to an object and placing the object at each of plural locations and orientations that are known with respect to the motion reference frame of the object. The location of the target(s) with respect to the object need not be known. An image of the object and target is generated while the object is at each of those locations/orientations. From each those images, the method determines the location/orientation of the target with respect to the reference frame of the camera. The method then calls for determining the calibration relationship between the reference frame of motion of the object and the camera reference frame as a function of the locations/orientations of the object with respect to the motion reference frame of the object and the locations/orientations of the target in the corresponding images with respect to the reference frame of the camera.
Claims It will be appreciated, of course, that the embodiments described above are merely examples of the invention and that other embodiments incorporation modifications thereto fall within the scope of the invention. For example, it will be appreciated that the calibration relationship between a camera and the moveable object (e.g., motion stage) need not encompass all of the parameters discussed above but may be limited, for example, to one or more of them, e.g., a calibration relationship may be limited to scale along each axis and the relative angle of the reference frames. By way of further example, it will be appreciated that, although much of the discussion herein is directed to cameras, the teachings herein apply equally to other image acquisition devices as well. In view of the foregoing, what we claim is: ##SPC1## 1. A method of determining a calibration relationship between a reference frame of a motion stage and a reference frame of each of plural image acquisition devices that generate images of that stage, the method comprising the steps of: A. placing a calibration plate on the motion stage, the calibration plate including plural targets, where each target (i) as a known location (w.sub.xi,w.sub.yi,) on the calibration plate, where at least one target is in the field of view of each image acquisition device, and where the motion stage is at a first known location/orientation (m.sub.x,m.sub.y, .theta.).sub.j, where (j)=1, and generating with each of the image acquisition devices a first image of the motion stage; B. determining a location (i.sub.x,i.sub.y where (j)=1, of the target in each of those first images; C. moving the motion stage to one or more other known locations/orientations (m.sub.x,m.sub.y,.theta.).sub.j, where (j)>1. such that at least one target is in the field of view of each image acquisition device, and generating with each image acquisition device additional images of the motion stage; D. determining locations (i.sub.x,i.sub.y).sub.ij, where (j)>1, of the target in each of those additional images; and E. determining a calibration relationship between the reference frame of the motion stage and the reference frames of the image acquisition devices as a function of (i) the known locations/orientations of the motion stage (m.sub.x,m.sub.y,.theta.).sub.j, where (j).gtoreq.1, (ii) the locations (i.sub.x,i.sub.y).sub.ij, where (j).gtoreq.1, of the targets in the corresponding images, and (iii) the known locations (w.sub.xi,w.sub.yi) of the targets on the calibration plate, wherein step (E) comprises the step of determining the calibration relationship by minimizing an error E.sub.ij between known locations/orientations of the motion stage and estimates thereof in accord with the mathematical relationship ##EQU10## where (m.sub.x,m.sub.y,.theta.).sub.j represents the known motion stage locations/orientations, (i.sub.x,i.sub.y).sub.ij represents locations of the targets in the images, (w.sub.xi,w.sub.yi) represents the known locations of each target on the calibration plate, G.sub.i (u,v), H(.sub.i (u,v) represent a lens distortion correction functions mapping coordinates (u,v) in an image to an orthonormal image coordinate system (ix,iy), P.sub.xi,P.sub.yi represent a position of target (i) in motion stage coordinates when the motion stage is at (x=0,y=0,.theta.=0), .alpha..sub.i, .beta..sub.i represent pixel width and height for camera field of view i, U and V represent the cosine and sine, respectively, of each image acquisition device's coordinate frame, (O.sub.xi,O.sub.yi) represents a physical position corresponding to specified location for camera field of view i. 2. A method of determining a calibration relationship between a reference frame of a motion stage and a reference frame of each of plural image acquisition devices that generate images of that stage, the method comprising the steps of: A. placing a calibration plate on the motion stage, the calibration plate including plural targets, where each target (i) as a known location (w.sub.xi,w.sub.yi) on the calibration plate, where at least one target is in the field of view of each image acquisition device, and where the motion stage is at a first known location/orientation (m.sub.x,m.sub.y,.theta.).sub.j, where (j)=1, and generating with each of the image acquisition devices a first image of the motion stage; B. determining a location (i.sub.x,i.sub.y).sub.ij, where (j)=1, of the target in each of those first images; C. moving the motion stage to one or more other known locations/orientations (m.sub.x,m.sub.y,.theta.).sub.j, where (j)>1, such that at least one target is in the field of view of each image acquisition device, and generating with each image acquisition device additional images of the motion stage; D. determining locations (i.sub.x,i.sub.y).sub.ij, where (j)>1, of the target in each of those additional images; and E. determining a calibration relationship between the reference frame of the motion stage and the reference frames of the image acquisition devices as a function of (i) the known locations/orientations of the motion stage (m.sub.x,m.sub.y,.theta.).sub.j, where (j).ltoreq.1, (ii) the locations (i.sub.x, i.sub.y).sub.ij, where (j).gtoreq.1, of the targets in the corresponding images and (iii) the known locations (w.sub.xi,w.sub.yi) of the targets on the calibration plate, wherein step (E) comprises the step of determining the calibration relationship by minimizing an error E.sub.ij between known locations/orientations of the motion stage and estimates thereof in accord with the following mathematical relationship ##EQU11## where (m.sub.x,m.sub.y,.theta.).sub.j represents the known motion stage locations/orientations, (i.sub.x,i.sub.y).sub.ij represents locations of the targets in the images, (w.sub.xi,w.sub.yi) represents the known locations of each target on the calibration plate, (x.sub.c, y.sub.c, .theta..sub.c) represent unknown locations/orientations of the calibration plate with respect to the motion stage, .alpha..sub.i, .beta..sub.i represent pixel width and height for camera field of view i, U and V represent the cosine and sine, respectively, of each image acquisition device's coordinate frame, (O.sub.xi,O.sub.yi) represents a physical position corresponding to specified location for camera field of view i. 3. A method according to claim 2, wherein step (E) comprises the steps of iteratively (i) using gradient descent to vary .theta..sub.c, and (ii) determining the calibration relationship by solving for the error E.sub.ij at each of those values of .theta..sub.c. 