Communication is an important issue in man-to-robot interaction. Signs can be used to interact with machines by providing user instructions or commands. Embodiment of the present invention include human detection, human body parts detection, hand shape analysis, trajectory analysis, orientation determination, gesture matching, and the like. Many types of shapes and gestures are recognized in a non-intrusive manner based on computer vision. A number of applications become feasible by this sign-understanding technology, including remote control of home devices, mouse-less (and touch-less) operation of computer consoles, gaming, and man-robot communication to give instructions among others. Active sensing hardware is used to capture a stream of depth images at a video rate, which is consequently analyzed for information extraction.
Claims What is claimed is: 1. A computer based method for man-machine interaction comprising the steps of: receiving a digital image of a target, the digital image comprising depth data associated with image pixels, the depth data corresponding to the distance to a camera capturing the digital image; determining a location of a hand of the target within the pixels of the image based on the depth data associated with each of the pixels and a shape of the target; pre-processing the digital image of the target by matching a candidate shape of the target captured in the digital image with a stored shape image from a plurality of shape images by comparing one or more of the plurality of stored shape images with the candidate shape, wherein comparing includes forming a vector with the depth data associated with each of the pixels associated with the candidate shape and computing a match based on a matrix of projection coefficient vectors representing the plurality of stored shape images; and matching a candidate shape of the hand by generating a skeleton representation of the hand from the image pixel data of the location of the hand and determining one or more hand attributes from the skeleton representation, including at least a number of fingers. 2. The method of claim 1, wherein matching the candidate shape of the hand further comprises: determining an edge of the hand in the determined location of the image; applying a pixel distance transform to the image with respect to the edge of the hand to compose a candidate edge image, wherein the one or more of the plurality of stored shape images include edge pixel distance transforms of the images for comparing with the candidate shape based on a projection of the candidate shape image over the stored shape images to determine a similarity value based on the correlation between the candidate edge image and the distance transforms of the stored images. 3. The method of claim 1, wherein determining a location of a hand of the target further comprises: dividing the digital image in a plurality of predetermined regions; associating a predetermined region with a location of the hand; and determining one or more regions associated with at least one of a location of the target's head or a location of the target's torso, wherein the one or more of the plurality of stored shape images for comparing with the candidate shape include region information defining hand locations associated with the shape. 4. A computer based method for man-machine interaction comprising the steps of: receiving a digital image of a target, the digital image including depth data associated with each pixel and being indicative of the distance to a camera capturing the digital image; pre-processing the digital image to determine a location of at least a hand and at least one of a head or a torso of the target based on the depth data associated with each pixel; matching a shape of the hand with one of a plurality of stored hand shape images to provide a matched shape associated with the digital image, wherein matching includes forming a vector with the depth data associated with each of the pixels associated with the location of the hand and computing a match based on a matrix of projection coefficient vectors representing the plurality of stored hand shape images; creating a candidate image data object including information associated with the location of a hand with respect to one of a head or a torso of the target and information indicating the matched shape associated with the digital image; matching a gesture captured by one or more digital images with a template gesture by comparing the candidate image data object with one ore more stored gesture profiles for template gestures, the gesture profiles including hand to body part location information and hand shape information for comparing with the candidate image data object; and matching a candidate shape of the hand by generating a skeleton representation of the hand from the image pixel data of the location of the hand and determining one or more hand attributes from the skeleton representation, including at least a number of fingers. 5. The method of claim 4, wherein pre-processing includes a training process comprising: performing a principal component analysis of a training set of images with known poses of the target to derive a derive a projection matrix; and determining a pose of the target in the digital image by projecting a vector corresponding to a candidate pose of the target based on the projection matrix and finding a nearest neighbor in the training set with a known pose. 6. The method of claim 4, wherein the candidate image data object further comprises a palm information section and a finger information section, the palm information section comprising shape information, orientation information, and trajectory information, the finger information section including number of fingers and direction of the fingers. 7. The method of claim 4 wherein the digital image of the target comprises hybrid depth-image information. 8. The method of claim 7, wherein pre-processing includes analyzing the hybrid depth-image information to determine location based on distance of the target to an image capturing system. 9. The method of claim 4, wherein matching the gesture captured by one or more digital images with a template gesture further comprises comparing values associated with one or more attributes from the group consisting of number of hands used, number of fingers used, distance between fingers, hand location, hand location variation, shape of hand information, and finger information. 10. The method of 9, wherein the shape of hand information further comprises one or more attributes from the group consisting of number of fingers, finger identifiers, finger shapes, and finger orientations. 11. A computer based method for human-machine interaction comprising: matching a hand shape captured in one or more digital images with a plurality of hand shape patterns to determine a candidate gesture hand shape, each of the one or more digital images including depth data associated with each pixel indicative of the distance to a camera capturing the digital images, wherein matching includes forming a vector with the depth data associated with each of the pixels and computing a match based on a matrix of projection coefficient vectors representing the plurality of stored hand shape patterns; determining a trajectory curve of the hand from the one ore more digital images; matching the trajectory curve of the hand with a plurality of trajectory curve templates to determine a candidate gesture motion; determining a gesture corresponding to the candidate gesture hand shape and the candidate gesture motion by comparing the candidate gesture hand shape and the candidate gesture motion with a plurality of gesture profiles with associated gesture hand shapes and gesture motions; and matching a candidate shape of the hand by generating a skeleton representation of the hand from the image pixel data of the location of the hand and determining one or more hand attributes from the skeleton representation, including at least a number of fingers. 12. The method of claim 11, further comprising: determining whether the hand shape captured in the one or more digital images is still; and in response to determining the hand shape to be still matching the hand shape with a plurality of hand shape patterns corresponding to still gestures to determine the gesture. 13. A system for human-machine interaction comprising: an image data pre-processing module for receiving frames of image data comprising human image information, each of the frames of image data including depth data associated with each pixel indicative of the distance to a camera that captured the frame, the image pre-processing module configured to determine locations of one or more body parts of the human in the frames based on the depth data associated with each pixel; a shape matching module coupled to the image data pre-processing module for receiving the determined locations and configured to match a shape of at least one of the body parts by comparing information associated with the image data with stored shape profiles in a shape database, wherein comparing includes forming a vector with the depth data associated with each of the pixels associated with a determined location of one or more body parts and computing a match based on a matrix of projection coefficient vectors representing the plurality of stored body part shape images; and a gesture matching module coupled to the image data-preprocessing module and the shape matching module for receiving shape and location information, the gesture matching module configured to match a gesture of the at least one body part by comparing gesture attributes of the at least one body part with gesture profiles stored in a gesture database, the gesture attributes comprising values associated with body part locations, and shape information, the gesture matching module further comprising a hand shape matching module configured to match a candidate shape of the hand by generating a skeleton representation of the hand from the image pixel data of the location of the hand and to determine one or more hand attributes from the skeleton representation, including at least a number of fingers. 14. The computer based system of claim 13, further comprising an image data capturing system, the image data capturing system configured to capture hybrid image-depth information of a scene within a range of depths from the image data capturing system. 15. The computer based system of claim 14, wherein the image data capturing system comprises a time-of-flight camera configured to capture the hybrid image-depth information along a single optical axis. 16. The computer based system of claim 13, further comprising a data storage system coupled to the shape matching module and the gesture matching module, the data storage system comprising the shape database and the gesture database. 17. The computer based system of claim 13, wherein the coupling between the image data pre-processing module, the shape matching module, and the gesture matching module comprises a random access memory device. 18. A computer readable media for human-machine interaction comprising: program instructions for receiving a digital image of a target, the digital image including depth data associated with each pixel indicative of the distance to a camera that captured the digital image; program instructions for pre-processing the digital image to determine a location of at least a hand and at least one of a head or a torso of the target based on the depth data associated with each pixel; program instructions for matching a shape of the hand with one of a plurality of stored hand shape images to provide a matched shape associated with the digital image, wherein the program instructions for matching include program instructions for forming a vector with the depth data associated with each of the pixels associated with the location of the hand and for computing a match based on a matrix of projection coefficient vectors representing the plurality of stored hand shape images; program instructions for creating a candidate image data object including information associated with the location of a hand with respect to one of a head or a torso of the target and information indicating the matched shape associated with the digital image; program instructions for matching a gesture captured by one or more digital images with a template gesture by comparing the candidate image data object with one ore more stored gesture profiles for template gestures, the gesture profiles including hand to body part location information and hand shape information for comparing with the candidate image data object; and program instructions for matching a candidate shape of the hand by generating a skeleton representation of the hand from the image pixel data of the location of the hand and determining one or more hand attributes from the skeleton representation, including at least a number of fingers. 19. A computer based system for human-machine interaction comprising: means for receiving a digital image of a target, the digital image including depth data associated with each pixel indicative of the distance to a camera that captured the digital image; means for pre-processing the digital image to determine a location of at least a hand and at least one of a head or a torso of the target based on the depth data associated with each pixel; means for matching a shape of the hand with one of a plurality of stored hand shape images to provide a matched shape associated with the digital image, wherein the means for matching include means for forming a vector with the depth data associated with each of the pixels associated with the location of the hand and means for computing a match based on a matrix of projection coefficient vectors representing the plurality of stored hand shape images; means for creating a candidate image data object including information associated with the location of a hand with respect to one of a head or a torso of the target and information indicating the matched shape associated with the digital image; and means for matching a gesture captured by one or more digital images with a template gesture by comparing the candidate image data object with one ore more stored gesture profiles for template gestures, the gesture profiles including hand to body part location information and hand shape information for comparing with the candidate image data object; and means for matching a candidate shape of the hand by generating a skeleton representation of the hand from the image pixel data of the location of the hand and determining one or more hand attributes from the skeleton representation, including at least a number of fingers. 20. A computer based method for man-machine interaction comprising the steps of: receiving a digital image of a target having a hand, the digital image comprising depth data associated with image pixels, the depth data corresponding to the distance from the target to a camera capturing the digital image; determining a location of a hand of the target within the pixels of the image based on the depth data associated with each of the pixels; and generating a skeleton representation of the hand from the image pixel data of the location of the hand, wherein generating includes determining characteristics of a first skeleton representation of the hand and applying a rule to the determined characteristics to delete at least one line segment in the first skeleton representation resulting in a second skeleton representation, wherein generating includes: determining an end point and an intersection point for a line in the first skeleton representation of the hand; determining the distance from the end point to the intersection point of the line; comparing the distance to a distance threshold characterizing a minimum distance for a finger; and deleting the line in the first skeleton representation based on the comparison to the distance threshold thereby removing a false finger skeleton line and resulting in a second skeleton representation. 21. The method of claim 20, further comprising: determining one or more hand attributes from the second skeleton representation, including at least a number of fingers; and comparing determined hand attributes with stored hand attribute profiles to recognize a hand sign. 22. The method of claim 21, wherein the hand attribute profiles include a palm object describing attributes including area, number of fingers, and location with respect to a target's torso. 23. The method of claim 21, wherein determining one or more hand attributes from the second skeleton representation further includes determining an orientation of a skeleton line in the second skeleton representation. 