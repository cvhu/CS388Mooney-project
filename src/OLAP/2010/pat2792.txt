A method of communicating physical human interactions over a communications network can include detecting physical movement of a user, generating data specifying the physical movement, and determining at least one action indicated by the data. The method further can include transmitting the action over a communications network to a receiving system and simulating the action in the receiving system.
Claims What is claimed is: 1. A method of bi-directionally communicating physical human interactions between users over a communications network comprising: performing a first action on a first model by a first user located at a first interaction system, said first interaction system including one or more sensors, one or more actuators, and a first message transmission module, said first model representing at least a portion of a human body including at least one among a human head, a human face, a human back and an entire human body, wherein said first model incorporates one or more sensors disposed at various portions or locations within or on the first model, wherein the first action of the first user includes at least one body movement of the first user and a change in facial expression of the first user, and wherein the at least one body movement and the change in facial expression are selectable by the first user through a visual interface; detecting portions or locations within or on the first model to which the first user applied force and an amount of force applied over time by each sensor, each sensor being configured to generate and send data to the first message transmission module when a force is detected, the generated data specifying a duration of the force detected, the amount of the force detected, and the portion or location within or on the first model to which the force was applied; the first message transmission module collecting and analyzing the data generated by each sensor and determining the first action intended by said first user; the first message transmission module converting the data to markup language formatted data and encoding the data into one or more messages for transmitting the determined action over the communications network to a second interaction system, said second interaction system being communicatively linked with the first interaction system via the communications network, said second interaction system including one or more sensors, one or more actuators, and a second message transmission module; the second message transmission module receiving and interpreting the one or more messages by processing the markup language formatted data encoded in the one or more messages to determine the first action specified by the one or more messages; simulating the first action by performing said first action on a second user at the second interaction system using a second model by activating one or more actuators incorporated in the second model, said second model representing at least said portion of said human body; performing a second action on the second model by the second user in response to the first action, wherein said second model incorporates one or more sensors disposed at various portions or locations within or on the second model; generating data from activated sensors within or on the second model specifying the second action and send the data to the second message transmission module; the second message transmission module collecting and analyzing the data generated by each of the activated sensor within or on the second model and determining the second action intended by said second user; the second message transmission module converting the data to markup language formatted data and encoding the data into one or more messages for transmitting the determined second action over the communications network to the first interaction system; and simulating the second action by performing said second action on the first user at the first interaction system using the first model, wherein said first model incorporates the one or more actuators. 2. The method of claim 1, wherein the markup language formatted data specifies at least one actuator movement to be implemented by the second model and an amount of force to be applied in the at least one actuator movement. 3. The method of claim 1, said simulating step further comprising the step of translating the action into instructions for activating at least one actuator; and activating the at least one actuator in accordance with the instructions. 4. The method of claim 1, wherein said generated data specifies a time when a force was detected, the amount of said force, and a location on said human body to which said force was applied. 5. The method of claim 1, wherein said action intended by said first user includes at least one among an embrace, a slap on the back, and a pat on the back. 6. The method of claim 1, further comprising: providing a graphical user interface, within said graphical user interface said first user can select human actions or processing tasks, wherein said human actions include at least one among "touch the face", "touch arm", and "embrace" and said processing tasks include at least one of "opening an audio channel" and "opening a video channel". 7. A system for bi-directionally communicating physical human interactions between users over a communications network comprising: a first interaction system and a second interaction system communicatively linked via the communications network; wherein the first interaction system includes: a first model upon which a first action is performed by a first user, the first model representing a portion of a human body representing at least one among a human head, a human face, a human back and an entire human body; at least one sensor disposed at various portions or locations within or on the first model and configured to detect physical movement of the first user, wherein the physical movement of the first user includes at least one body movement of the first user and a change in facial expression of the first user, wherein portions or locations within or on the first model to which the first user applied force and an amount of force applied over time are detected by each sensor, each sensor being configured to generate and send data when a force is detected, the generated data specifying a during of the force detected, the amount of force detected, and the portion or location to which force was applied; a first message transmission module for collecting and analyzing the data generated by each sensor and determining the first action intended by the first user, and for converting the data to markup language formatted data and encoding the data into one or more messages for transmitting the determined first action over the communications network to the second interaction system, wherein the first message transmission module has a visual interface through which the first user can select the at least one body movement and the change in facial expression; and at least one actuator incorporated within or on the first model to simulate any action received at the first interaction system via the communications network; the second interaction system includes: a second message transmission module for receiving and interpreting the one or more messages to determine the first action specified by the one or more messages; a second model incorporating at least one actuator configured to simulate the first action to a second user at the second interaction system, the second model representing at least the portion of the human body; and at least one sensor disposed at various portions or locations within or on the second model, wherein the at least one sensor detects a second action of the second user in response to the first action. 8. A computer readable storage medium, having stored thereon a computer program having a plurality of code sections executable by a machine for causing the machine to perform the steps of: performing a first action on a first model by a first user located at a first interaction system, said first interaction system including one or more sensors, one or more actuators, and a first message transmission module, said first model representing at least a portion of a human body including at least one among a human head, a human face, a human back and an entire human body, wherein said first model incorporates one or more sensors disposed at various portions or locations within or on the first model, and wherein the first action of the first user includes at least one body movement of the first user and a change in facial expression of the first user, and wherein the at least one body movement and the change in facial expression are selectable by the first user through a visual interface; detecting portions or locations within or on the first model to which the first user applied force and an amount of force applied over time by each sensor, each sensor being configured to generate and send data to the first message transmission module when a force is detected, the generated data specifying a duration of the force detected, the amount of the force detected, and the portion or location within or on the first model to which the force was applied; the first message transmission module collecting and analyzing the data generated by each sensor and determining the first action intended by said first user; the first message transmission module converting the data to markup language formatted data and encoding the data into one or more messages for transmitting the determined action over a communications network to a second interaction system, said second interaction system being communicatively linked with the first interaction system via the communications network, said second interaction system including one or more sensors, one or more actuators, and a second message transmission module; the second message transmission module receiving and interpreting the one or more messages by processing the markup language formatted data encoded in the one or more messages to determine the first action specified by the one or more messages; simulating the first action by performing said first action on a second user at the second interaction system using a second model by activating one or more actuators incorporated in the second model, said second model representing at least said portion of said human body; performing a second action on the second model by the second user in response to the first action, wherein said second model incorporates one or more sensors disposed at various portions or locations within or on the second model; generating data from activated sensors within or on the second model specifying the second action and send the data to the second message transmission module; the second message transmission module collecting and analyzing the data generated by each of the activated sensor within or on the second model and determining the second action intended by said second user; the second message transmission module converting the data to markup language formatted data and encoding the data into one or more messages for transmitting the determined second action over the communications network to the first interaction system; and simulating the second action by performing said second action on the first user at the first interaction system using the first model, wherein said first model incorporates the one or more actuators. 