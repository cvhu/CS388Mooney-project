A system for estimating orientation of a target based on real-time video data uses depth data included in the video to determine the estimated orientation. The system includes a time-of-flight camera capable of depth sensing within a depth window. The camera outputs hybrid image data (color and depth). Segmentation is performed to determine the location of the target within the image. Tracking is used to follow the target location from frame to frame. During a training mode, a target-specific training image set is collected with a corresponding orientation associated with each frame. During an estimation mode, a classifier compares new images with the stored training set to determine an estimated orientation. A motion estimation approach uses an accumulated rotation/translation parameter calculation based on optical flow and depth constrains. The parameters are reset to a reference value each time the image corresponds to a dominant orientation.
Claims What is claimed is: 1. A computer based method for estimating a real-time orientation measure for a target using depth video image data, the method comprising the steps of: receiving as input a feed of depth video frames, the depth video frames capturing the target in real-time and including depth pixel data; storing a target-specific training set of depth video frames during an initial training period; identifying frames of the target-specific training set of depth video frames capturing the target in a dominant orientation by analyzing the target-specific training set of depth video frames; comparing, based in appearance, a current depth video frame with the target-specific training set of depth video frames to determine whether the current depth video frame includes the target in the dominant orientation; determining a current orientation measure based in part on depth data corresponding to the current depth video frame, the current orientation measure corresponding to a current orientation of the target captured in the current depth video frame; and setting the current orientation measure to a reference orientation measure in response to determining that the current depth video frame includes the target in the dominant orientation. 2. The method of claim 1, wherein analyzing the target-specific training set of depth video frames includes performing a Parzen-window based PDF and determining the mode of the PDF. 3. The method of claim 1, wherein analyzing the target-specific training set of depth video frames includes segmenting each depth video frame to determine a segment of the frame that includes image data corresponding to the target. 4. The method of claim 3, wherein analyzing the target-specific training set of depth video frames further includes tracking the segment position from frame to frame based on one of an elliptic fitting method or a Mean shift algorithm. 5. The method of claim 1, wherein comparing, based in appearance, further comprises: determining a projection matrix and a pose curve using PCA analysis of the target-specific training set of depth video frames; projecting the current depth video frame onto a set of multidimensional eigenspaces based on the projection matrix; and estimating an orientation based on the projected current depth video frame. 6. The method of claim 5, wherein estimating the orientation includes finding a nearest neighbor based on the projection of the current depth video frame and points of the pose curve. 7. The method of claim 5, wherein estimating the orientation includes performing a linear interpolation between points in the pose curve corresponding to the projected current depth video frame. 8. The method of claim 1, wherein determining the current orientation measure further comprises: estimating an optical flow between feature points in a previous depth video frame and the current depth video frame to determine two-dimensional motion fields; recovering three-dimensional rotation and translation parameters between the previous depth video frame and the current depth video frame using the two-dimensional motion fields and depth constrains based on the depth pixel data corresponding to the feature points; and setting the current orientation measure to an accumulated orientation value based on an orientation measure for the previous depth video frame and the three-dimensional rotation and translation parameters. 9. The method of claim 1, wherein the target is a driver's head and the orientation is a head pose. 10. A computer readable storage medium for estimating a real-time orientation measure for a target using depth video image data, comprising a computer program that when executed by a computer processor implements the steps of: receiving as input a feed of depth video frames, the depth video frames capturing the target in real-time and including depth pixel data; storing a target-specific training set of depth video frames during an initial training period; identifying frames of the target-specific training set of depth video frames capturing the target in a dominant orientation by analyzing the target-specific training set of depth video frames; comparing, based in appearance, a current depth video frame with the target-specific training set of depth video frames to determine whether the current depth video frame includes the target in the dominant orientation; determining a current orientation measure based in part on depth data corresponding to the current depth video frame, the current orientation measure corresponding to a current orientation of the target captured in the current depth video frame; and setting the current orientation measure to a reference orientation measure in response to determining that the current depth video frame includes the target in the dominant orientation. 11. A system for estimating a real-time orientation measure for a target using depth video image data, the system comprising: means for receiving as input a feed of depth video frames, the depth video frames capturing the target in real-time and including depth pixel data; means for storing a target-specific training set of depth video frames during an initial training period; means for identifying frames of the target-specific training set of depth video frames capturing the target in a dominant orientation by analyzing the target-specific training set of depth video frames; means for comparing, based in appearance, a current depth video frame with the target-specific training set of depth video frames to determine whether the current depth video frame includes the target in the dominant orientation; means for determining a current orientation measure based in part on depth data corresponding to the current depth video frame, the current orientation measure corresponding to a current orientation of the target captured in the current depth video frame; and means for setting the current orientation measure to a reference orientation measure in response to determining that the current depth video frame includes the target in the dominant orientation. 