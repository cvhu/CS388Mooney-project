A method and apparatus for fusion of three dimensional data due to an active optical triangulation based vision sensor, and two-dimensional data obtained from an independent TV camera. The two sensors are aligned so that their view volumes are partially overlapped. In the course of a calibration process, a planar target is placed and depressed through the common view volume of the two sensors. The illuminant of vision sensor projects a stripe onto the target. As the stripe is traversed across the target incrementally, it is imaged at every position not only by the camera of the vision sensor, but also by the independent TV camera. The calibration process yields a table which connects the resolution cells of the TV camera to a set of rays whose equations are established in the coordinate measurement system of the three dimensional vision sensor. The calibration table is subsequently used to inter-relate points in either sensory space via their connecting rays.
Claims What is claimed is: 1. A calibration process with a self-scanned active three dimensional optical triangulation-based sensor and an independent TV camera and a planar target to provide a fused sensory source for subsequent complete data fusion of a scene imaged by both its constituent sensory systems, and is not constrained by absence of spatial or spectral structures in the scene, said calibration process comprising the steps: of aligning two sensors with view volumes so that their view volumes partially overlap; imaging a laser-projected stripe of a self-scanned 3D sensor at every step by both sensory systems as the stripe is being traversed at steps commensurate with a desired lateral resolution across a target comprising a planar rectangular slab; processing imaged lasers stripes to yield a set of corresponding least squares fitted line segments in a first sensory space and a second sensory space arising from least squares line fitting of preprocessed and Hough-like transformed points between leading and trailing edge elements of the imaged stripe at every step and projection of the said edge elements onto said lines in both said spaces; sampling corresponding line segments at equal numbers of equidistant intervals to a desired vertical resolution to yield two ordered point sets that are a mapping of one another; recording coordinates of 3D points against those of corresponding points in said second space; repeating all said steps as the target is being depressed through the overlapping view volume; and least squares line fitting in three space coordinate measurement system of said first space, said second space having resolution cells, three space points corresponding to each resolution cell of said second space; viewing line equations as those of rays connecting said second space resolution cells to their corresponding three space points; and storing thereafter said equations along with their corresponding second space resolution cells in a data fusion table for subsequent data fusion during run-time. 2. A calibration process as defined in claim 1, including the step of producing subsequent fusion of data from subpixel to arbitrary coarser resolutions. 3. A calibration process as defined in claim 1, including the step of controlling said independent TV camera in said fused sensory source under ambient illumination provided by a source other than said self-scanned 3D sensor. 4. A calibration process as defined in claim 1, including the step of introducing an additional self-scanned 3D sensor for extending a data fusion region. 5. A calibration process as defined in claim 1, including the step of: providing a plurality of independent cameras with adjacent 3D sensors, said independent cameras and said adjacent sensors having each a field of view; and spanning each field of view by the respective camera over the field of view of the adjacent sensor for obtaining additional data on intensity images. 6. A calibration process as defined in claim 1, wherein said fused sensory source is formed from a single plane of light sensor and a TV camera moving in tandem with one another. 7. A calibration process as defined in claim 1, including the step of producing a test pattern for registering a single plane of light optical triangulation based 3D sensor against a supporting robot arm. 8. A calibration process as defined in claim 1, including the step of controlling said independent TV camera in said fused sensory source under ambient illumination provided by said self-scanned 3D sensor. 9. A calibration process as defined in claim 1, including the step of producing a test pattern for registering a single plane of light optical triangulation based 3D sensor against another self-scanned 3D sensor. 10. A calibration process as defined in claim 1, including the step of extrapolating for extending a data fusion region along a laser projected stripe of said self-scanned 3D sensor. 11. A calibration process is defined in claim 10, wherein the data fusion region is extended in directions along said laser projected stripe and across said target, said target comprising a circular disc; least squares fitting circles in said first sensory space; least squares fitting an ellipse in said second sensory space, correspondence between the least squares fitted circles and the least squares fitted ellipse being to ends of line segments representing said imaged stripe. 12. A calibration process as defined in claim 1, wherein a flying spot based sensor is calibrated. 13. A calibration process as defined in claim 12, including the step of producing equations of rays extending to polynomials dependent on uncorrected aberrations. 