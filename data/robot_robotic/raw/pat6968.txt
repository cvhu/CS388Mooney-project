The relational robotic controller (RRC) is a robotic control devise made up of a stack of relational correlation sequencers (RCS). The RRC controls the tasks (including sound generation, memory storage and retrieval tasks) performed by a robot. The RRC is programmed to perform all tasks relative a self location and identification task, performed by a nodal map, known as the self nodal map, and associated with one of the RCS that make up the RRC. The self-nodal map, operating simultaneously while other nodal maps are performing other tasks, acts as a recording monitor that senses data from pressure transducers, cameras, and microphones, relative to the self of the robot. When a phoneme-sound generating, auditory-microphone, and memory storage and retrieval RCS capability is coupled with a visual-camera capability, a RRC controlled robot may be trained to achieve coordinated camera visualization, memory recall, and verbal communication.
Claims What is claimed is: 1. An apparatus for teaching a device how to perform a second set of tasks based upon a related first set of tasks, including a set of nodal maps, including at least one self nodal map, wherein said nodal maps and said self nodal map are comprised of a set of cause vectors and effect vectors; a task selector coupled to said set of nodal maps; a sequencer coupled to said set of nodal maps, wherein said sequencer selects a sequence of said cause vectors used to navigate from said a first said effect vector to a second said effect vector; and a set of sensors. 2. An apparatus as in claim 1, wherein said apparatus generates a sequence of control vectors during each frame period such that said sequence is an "end point planning" process performed at the beginning of a task that is susceptible to change at anytime in the course of performance. 3. An apparatus as in claim 2, wherein the end point of said sequence is associated with the end point of said task. 4. An apparatus as in claim 1, wherein said set of sensors includes a set of pressure transducers and at least one of the following: a set of optical sensors, a set of microphones, a wet spectrometer and a dry spectrometer. 5. An apparatus as in claim 1, wherein said nodal maps included in said set of nodal maps are Kohonen self-organizing maps. 6. An apparatus as in claim 1, wherein said set of cause vectors is time invariant. 7. An apparatus as in claim 1, wherein said set of cause vectors are inputs to the motor controls of a robot. 8. An apparatus as in claim 1, wherein said set of nodal maps and said "self" nodal map mirror the three dimensional Euclidean space around said device. 9. An apparatus as in claim 1, wherein said sequencer is trained to navigate through said mirrored three dimensional space on the basis of field data from said effect vectors coupled to said mirrored three dimensional space. 10. An apparatus as in claim 1, wherein said self nodal map is used to record pressure transducer data during the performance of said second set of tasks. 11. An apparatus as in claim 1, wherein the self nodal map is used to record camera data, while said device is performing a second set of tasks. 12. An apparatus as in claim 1, wherein said self nodal map is used to record microphone data during performance of said second set of tasks. 13. An apparatus as in claim 1, wherein said set of nodal maps correspond to a multi-dimensional function space, wherein said functions are phonemes. 14. An apparatus as in claim 13, wherein said sequencer navigates through said multidimensional function space using sound field inputs. 15. An apparatus as in claim 1, wherein said self nodal map corresponds to data from a set of pressure transducers distributed on the peripheral surface of said device. 16. An apparatus as in claim 15, wherein said effect vector mapping of said data from said set of pressure transducers programs the self location. 17. An apparatus as in claim 1, wherein said self nodal map corresponds to data from one or more optical devices. 18. An apparatus as in claim 17, wherein said one or more optical devices include cameras. 19. An apparatus as in claim 1, wherein said self nodal map corresponds to a data from one or more auditory inputs. 20. An apparatus as in claim 19, wherein said one or more auditory inputs include one or more microphones. 21. An apparatus as in claim 1, including a means for updating said set of nodal maps. 22. An apparatus as in claim 1, including a task generator coupled to said set of nodal maps and said self nodal map. 23. An apparatus as in claim 22, wherein said task generator includes a hierarchical structure for assignment of priorities to tasks generated by said task generator. 24. A method for teaching a device how to perform a second set of tasks based upon a related first set of tasks, including receiving a first set of signals from at least one sensor; mapping said signals on a nodal map included in a set of nodal maps, wherein said nodal map includes a set of cause vectors and effect vectors and at least one self nodal map; updating said nodal map in response to information received from said sensor; receiving a second set of signals from at least one sensor; and using information stored in said nodal map to generate a task trigger. 25. A method as in claim 24, wherein said step of receiving a first set of signals corresponds to the training of said nodal map. 26. A method as in claim 24, wherein said at least one sensor includes one or more of the following: an optical sensor, an auditory sensor, an olfactory sensor, a gustatory sensor and a tactile sensor. 27. A method as in claim 26, wherein said self nodal map records the data from said optical sensor. 28. An apparatus as in claim 26, wherein said one or more optical sensor includes cameras. 29. A method as in claim 26, wherein said "self" nodal map records the data from one or more auditory sensor. 30. A method as in claim 29, wherein said auditory sensor includes a microphones. 31. A method as in claim 24, wherein said step of mapping includes selecting a sequence of said cause vectors used to navigate from said a first said effect vector to a second said effect vector. 32. A method as in claim 24, wherein said self nodal map is used to record pressure transducer data during the performance of said second set of tasks. 33. A method as in claim 24, wherein said self nodal map is used to record camera data during the performance of said second set of tasks. 34. A method as in claim 24, wherein said self nodal map is used to record microphone data during the performance of said second set of tasks. 35. A method as in claim 24, wherein said set of nodal maps are Kohonen self-organizing maps. 36. A method as in claim 24, wherein, wherein said set of cause vectors is time invariant. 37. A method as in claim 24, wherein said set of cause vectors are inputs to the motor controls of a robot. 38. A method as in claim 24, wherein said self nodal map mirrors the three dimensional Euclidean space. 39. A method as in claim 24, wherein said set of nodal maps includes a multi-dimensional function space and said functions are phonemes. 40. A method as in claim 24, wherein said self nodal map includes data from a set of pressure transducers distributed on the peripheral surface of said device. 41. A method as in claim 40, wherein said effect vector mapping of said data from said of pressure transducers programs the self location. 42. A method as in claim 24, wherein said self nodal map corresponds to data from one or more optical devices. 43. A method as in claim 24, wherein said self nodal map corresponds to a data from one or more auditory inputs. 44. A method in claim 43, wherein said one or more auditory inputs include one or more microphones. 45. A method as in claim 43, wherein said effect vector mapping of said data from said set of pressure transducers programs the "self" location. 46. A method as in claim 24, wherein said step of mapping includes training a sequencer to navigate through a mirrored three dimensional space on the basis of q field data coupled to said mirrored three dimensional space. 47. A method as in claim 46, wherein said sequencer navigates through said multidimensional function space using sound field inputs. 48. A method as in claim 24, including a step of generating a hierarchy of tasks with associated priority levels. 