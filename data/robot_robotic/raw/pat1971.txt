A method for tracking a number of objects or object parts in image sequences utilizes a Bayesian-like approach to object tracking, computing, at each time a new image is available, a probability distribution over all possible target configurations for that time. The Bayesian-like approach to object tracking computes a probability distribution for the previous image, at time (t-1), is propagated to the new image at time (t) according to a probabilistic model of target dynamics, obtaining a predicted distribution at time (t). The Bayesian-like approach to object tracking also aligns the predicted distribution at time (t) with the evidence contained in the new image at time (t) according to a probabilistic model of visual likelihood.
Claims What is claimed is: 1. A method for tracking a number of objects or object parts in image sequences, comprising following a Bayesian-like approach to object tracking, computing, at each time a new image is available, a probability distribution over target configurations for that time, said Bayesian-like approach to object tracking comprising: computing a probability distribution for the previous image, at time (t-1), and propagating the probability distribution to the new image, at time (t), according to a probabilistic model of target dynamics, to obtain a predicted, or prior, distribution at time (t); and aligning the predicted distribution, at time (t), with the evidence contained in the new image, at time (t), according to a probabilistic model of visual likelihood, to obtain a posterior distribution at time (t); wherein aligning the predicted distribution, at time (t), of a hypothetic target configuration associated with a new frame by performing image analysis includes, identifying the image portion in which the target under consideration is expected to be visible under the current configuration, by using a shape rendering function, assigning a probability value to each pixel of the identified image portion, which is computed as a probability of the target under consideration being expected to be visible under the current configuration being occluded by another target, the probability of the target under consideration being expected to be visible under the current configuration being occluded by another target being derived from prior distributions and shape models of other targets, computing a degree of dissimilarity between visual features extracted from the identified image portion and a corresponding characterization, or appearance model, of the target, an importance, or weight, of the different pixels being calculated by the probability of the target under consideration being expected to be visible under the current configuration being occluded, adding, to the value of degree of dissimilarity, a further dissimilarity term per each other tracked target, each further dissimilarity term being computed in form of an expectation, under a prior distribution, of the dissimilarity value, the expectation of dissimilarity value being calculated on configurations that map closer to an image recording device than the target under analysis wherein the target, currently under consideration, being neglected, and calculating a distribution value, assigned to the hypothetic target configuration of the target under analysis, by multiplying a prior value with a negative exponential of an overall dissimilarity score. 2. The method according to claim 1, wherein said distribution value assigned to the hypothetic target configuration of the target under analysis is calculated recursively according to: q(x.sub.t.sup.k|z.sub.1:t).varies.f(z.sub.t|x.sub.t.sup.k,z.sub.1:t-1)q(x- .sub.t.sup.k|z.sub.1:t-1) where: x.sub.t.sup.k is the configuration of a scene at time t for a target k; z.sub.t is the image at time (t); q(x.sub.t.sup.k|z.sub.1:1) is a posterior distribution for each single target k, when multiple targets are present, at time (t); and q(x.sub.t.sup.k|z.sub.1:t-1) is the prior distribution at time (t); f(z.sub.t|x.sub.t.sup.k, z.sub.1:t-1) is a multi-target likelihood function which implements the occlusion process, according: .times..times..function..times..times..apprxeq..function..times..times..n- oteq..times..intg..function..times..times..times..function..times..times..- times.d ##EQU00009## where: L.sup.k(z.sub.t|w.sup.k), a foreground term, is the degree of dissimilarity between visual features extracted from the identified image region and a corresponding characterization, or appearance model, of the target; and .intg.L.sup.m(z.sub.t|w.sup.m|k)q(x.sub.t.sup.m|z.sub.1:t-1)dx.sub.t.sup.- m, a background term, is the further dissimilarity term per each other tracked target. 3. The method according to claim 2, wherein said foreground term, L.sup.k(z.sub.t|w.sup.k), calculates a dissimilarity score between the image part where target k is expected to be visible, and its appearance model by the following steps: the image region identified by the shape model for the hypothesis x.sub.t.sup.k under analysis is first calculated; pixel weights w.sup.k are computed within the image area; weighted appearance features are extracted from that region and compared to the appearance model of the target; and the comparison being made in terms of a distance function between extracted and modeled appearance features and defining the value of L.sup.k(z.sub.t|w.sup.k). 4. The method according to claim 2, wherein the background term, .intg.L.sup.m(z.sub.t|w.sup.m|k)q(x.sub.t.sup.m|z.sub.1:t-1)dx.sub.t.sup.- m,takes into account the occlusion evidence derived from the presence of target m.noteq.k, and is computed as the expectation of the foreground term of m under prior q(x.sub.t.sup.m|z.sub.1:t-1) when the configuration of k is blocked to hypothesis X.sub.t.sup.k under analysis; and in that, for each x.sub.t.sup.m, a foreground score, L.sup.m(z.sub.t|w.sup.m|k), is computed, using weight maps w.sup.m|k; the scores are modulated with the prior q(x.sub.t.sup.m|z.sub.1:t-1) and integrated. 5. The method according to claim 2, wherein the pixel weights and weight maps w.sup.k, w.sup.m|k are scalar images computed by attributing to each pixel of z a probability value in the range [0,1], the probability value being interpreted as the importance of that pixel when computing the likelihood score for target k. 6. The method according to claim 5, characterized in that the pixel weights and weight maps w.sup.k, w.sup.m|k are computed for each pixel u as follows: .function..noteq..times..times..intg..times.<.times..times..function..- times..times..times.d.times..times..times.<.times..infin..function..not- eq..times..times..intg..times.<.times..times..function..times..times..t- imes.d.times..times..times.<.times..times.<.times..infin. ##EQU00010## where x.sub.t.sup.m<.sub.ux.sub.t.sup.k identifies the set of configurations x.sub.t.sup.m which, according to associated shape rendering function, render closer to the image recording device in pixel u than configuration x.sub.t.sup.k, relation x.sub.t.sup.k<.sub.u.infin. is meant to be true when x.sub.t.sup.k is observable in pixel u. 7. The method according to claim 1, wherein the shape rendering function is a user-defined shape model of the target that is rendered onto the image for the specific configuration under analysis. 8. The method according to claim 1, wherein the computing of the degree of dissimilarity is user defined. 9. An apparatus for tracking a number of objects or object parts in image sequences comprising: a processor; said processor computing a probability distribution for the previous image, at time (t-1 ), and propagating the probability distribution to the new image, at time (t), according to a probabilistic model of target dynamics, to obtain a predicted, or prior, distribution at time (t); and said processor aligning the predicted distribution, at time (t), with the evidence contained in the new image, at time (t), according to a probabilistic model of visual likelihood, to obtain a posterior distribution at time (t); said processor aligning the predicted distribution, at time (t), of a hypothetic target configuration associated with a new frame by, identifying the image portion in which the target under consideration is expected to be visible under the current configuration, by using a shape rendering function, assigning a probability value to each pixel of the identified image portion, which is computed as a probability of the target under consideration is expected to be visible under the current configuration being occluded by another target, the probability of the target under consideration being expected to be visible under the current configuration being occluded by another target being derived from prior distributions and shape models of other targets, computing a degree of dissimilarity between visual features extracted from the identified image portion and a corresponding characterization, or appearance model, of the target, the importance, or weight, of the different pixels in this calculation being calculated by the probability of the target under consideration being expected to be visible under the current configuration being occluded, adding, to the value of degree of dissimilarity, a further dissimilarity term per each other tracked target, each further dissimilarity term being computed in form of an expectation, under a prior distribution, of the dissimilarity values, the expectation of dissimilarity values being calculated on configurations that map closer to an image recording device than the target under analysis wherein the target, currently under consideration, being neglected, and calculating a distribution value, assigned to the hypothetic target configuration of the target under analysis, by multiplying a prior value with the negative exponential of the overall dissimilarity score. 10. A recordable computer readable medium having a program recorded thereon, adapted to cause a process to be executed on a computer, the process comprising: computing a probability distribution for the previous image, at time (t-1 ), and propagating the probability distribution to the new image, at time (t), according to a probabilistic model of target dynamics, to obtain a predicted, or prior, distribution at time (t); and aligning the predicted distribution, at time (t), with the evidence contained in the new image, at time (t), according to a probabilistic model of visual likelihood, to obtain a posterior distribution at time (t); wherein aligning the predicted distribution, at time (t), of a hypothetic target configuration associated with a new frame by performing image analysis includes, identifying the image portion in which the target under consideration is expected to be visible under the current configuration, by using a shape rendering function, assigning a probability value to each pixel of the identified image portion, which is computed as a probability of the target under consideration is expected to be visible under the current configuration being occluded by another target, the probability of the target under consideration being expected to be visible under the current configuration being occluded by another target being derived from prior distributions and shape models of other targets, computing a degree of dissimilarity between visual features extracted from the identified image portion and a corresponding characterization, or appearance model, of the target, the importance, or weight, of the different pixels in this calculation being calculated by the probability of the target under consideration being expected to be visible under the current configuration being occluded, adding, to the value of degree of dissimilarity, a further dissimilarity term per each other tracked target, each further dissimilarity term being computed in form of an expectation, under a prior distribution, of the dissimilarity values, the expectation of dissimilarity values being calculated on configurations that map closer to an image recording device than the target under analysis wherein the target, currently under consideration, being neglected, and calculating a distribution value, assigned to the hypothetic target configuration of the target under analysis, by multiplying a prior value with the negative exponential of the overall dissimilarity score. 