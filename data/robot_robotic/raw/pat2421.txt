A surgical navigation system has a computer with a memory and display connected to a surgical instrument or pointer and position tracking system, so that the location and orientation of the pointer are tracked in real time and conveyed to the computer. The computer memory is loaded with data from an MRI, CT, or other volumetric scan of a patient, and this data is utilized to dynamically display 3-dimensional perspective images in real time of the patient's anatomy from the viewpoint of the pointer. The images are segmented and displayed in color to highlight selected anatomical features and to allow the viewer to see beyond obscuring surfaces and structures. The displayed image tracks the movement of the instrument during surgical procedures. The instrument may include an imaging device such as an endoscope or ultrasound transducer, and the system displays also the image for this device from the same viewpoint, and enables the two images to be fused so that a combined image is displayed. The system is adapted for easy and convenient operating room use during surgical procedures.
Claims What is claimed is: 1. A system for augmented viewing of a patient target site in a surgical setting by a user, comprising: a computer for storing scan data of the patient target region, and for constructing from the stored data, a three dimensional perspective volumetric image of the target site as seen from a selected orientation; a viewing device, the device having a first ocular instrument and a second ocular instrument in which the patient target site can be viewed, and in which the three dimensional perspective volumetric image can be projected, forming a fused image using the patient target site view and the three dimensional perspective volumetric image; a tracking device coupled to the computer for tracking the selected orientation of the viewing device; and a calibration module in the computer for placing the viewing device in the same coordinate system as the patient in response to tracking information received from the tracking device, wherein the selected orientation from which the volumetric image of the target is constructed by the computer is the selected orientation of the second ocular instrument. 2. The system of claim 1 wherein the patient target site can be viewed in the first ocular instrument and the three dimensional perspective volumetric image can be projected in the second ocular instrument. 3. The system of claim 1 wherein the patient target site can be viewed in the first and second ocular instrument and the three dimensional perspective volumetric image can be projected in the second ocular instrument. 4. The system of claim 1 wherein the patent target site can be viewed in the first and second ocular instrument and the three dimensional perspective volumetric image can be projected in the first and second ocular instrument. 5. The system of claim 1 wherein scan data is selected from a group comprising computed tomography, magnetic resonance imaging data, single photon emission computed tomography, x-ray data, positron emission data, ultrasound data, video data, and computed axial tomography data. 6. The system of claim 1 wherein an orientation of a surgical tool is projected on the three dimensional perspective volumetric image of the target site. 7. The system of claim 1 wherein a position of a surgical tool is projected on the three dimensional perspective volumetric image of the target site. 8. The system of claim 1 further comprising a laser scanner mounted on the viewing device for scanning a laser beam over a surface of the patient target region to create a laser pattern on the patient, and wherein the tracking device included two or more cameras for viewing the laser pattern on the patient from two or more different orientations. 9. The system of claim 8 wherein the calibration module is operative to match surface features of the patient, as determined from the volumetric scan data of the patient, with corresponding surface features, as determined from the laser pattern on the patient, to place the scan data in the same frame of reference as the patient. 10. The system in claim 8 wherein the calibration module is operative to determine, from the laser pattern on the patient as viewed by the two or more cameras, the selected orientation of the second ocular instrument. 11. The system of claim 8 wherein the two or more cameras includes a first pair of cameras to observe the patient surface image and a second pair of cameras to track a source of the laser beam. 12. The system of claim 1, further comprising a tracked surgical insertion device having an orientation and a position, the surgical insertion device projected on the three dimensional perspective volumetric image of the target site. 13. A method for augmented viewing of a patient target site in a surgical setting through a viewing device having a pair of ocular instruments, the method comprising: storing scan data of the patient target region in a computer, the scanned data having a scanned data coordinate system; tracking the position of the patient and the position and orientation of the viewing device; registering a patient coordinate system with the scanned data coordinate system; constructing a three dimensional perspective volumetric image of the target site using the stored scanned data with a computer processor to create said three dimensional perspective volumetric image from the position and orientation of the viewing device; and injecting the three dimensional perspective volumetric image in the viewing device such that a user sees both an actual image of the patient and the constructed three dimensional perspective volumetric image in the combined ocular instrument. 14. The method of claim 13 further comprising mounting a laser scanner on the viewing device for scanning a laser beam over a surface of the patient target region to create a laser pattern on the patient. 15. The method of claim 14 further comprising matching surface features of the patient, as determined from the volumetric scan data of the patient, with corresponding surface features, as determined from the laser pattern on the patient, to place the scanned data in the patient coordinate system. 16. The method of claim 13 further comprising identifying a field of view and orientation of the viewing device. 17. The method of claim 13 wherein scan data is selected from a group comprising computed tomography, magnetic resonance imaging data, single photon emission computed tomography, x-ray data, positron emission data, ultrasound data, video data, and computed axial tomography data. 18. A system for augmented stereotactic surgery, comprising: a set of scanned image data; a microscope having a microscopic field of view, the microscope having a first and a second ocular instrument; a laser scanning device aligned to project an outline of the microscopic field of view on a patient anatomy; a computer having a construction module and a registration module, the construction module configured for constructing a three dimensional perspective volumetric representation of the set of scanned image data, wherein the registration module is configured for aligning the outline of the microscopic field of view with the three dimensional perspective volumetric representation of the set of scanned image data; and a image injection device coupled to the microscope for injecting the three dimensional perspective volumetric representation of the set of scanned image data in the microscopic field of view. 19. The system of claim 18 wherein the intraoperative imagery of the microscopic field of view can be viewed in the first ocular instrument and the three dimensional perspective volumetric representation of the set of scanned data can be injected in the second ocular instrument. 20. The system of claim 18 wherein the intraoperative imagery of the microscopic field of view can be viewed in the first and second ocular instrument and the three dimensional perspective volumetric representation of the set of scanned data can be injected in the second ocular instrument. 21. The system of claim 18 wherein the intraoperative imagery of the microscopic field of view can be viewed in the first and second ocular instrument and the three dimensional perspective volumetric representation of the set of scanned data can be injected in the first and second ocular instrument. 22. The system of claim 18 wherein an orientation of a surgical tool is projected on the three dimensional perspective volumetric representation of the set of scanned data. 23. The system of claim 18 wherein a position of a surgical tool is projected on the three dimensional perspective volumetric representation of the set of scanned data. 24. The system of claim 18 further comprising a tracking device having two or more cameras for viewing the outline of the microscopic field of view from two or more different orientations wherein the two or more cameras includes a first pair of cameras to observe a patient surface image and a second pair of cameras to track a source of a laser beam originating from the laser scanning device. 25. An apparatus for combining a representation of a patient anatomy generated from preexisting image data with dynamic imagery, comprising: a surgical instrument having an optical field of view for dynamic viewing of the patient anatomy; a laser scanning device coupled to the surgical instrument and configured to project the surgical instrument's optical field of view on the patient anatomy; a computer capable of creating a three dimensional perspective volumetric image of the preexisting image data in registration with the optical field of view of the surgical device and the patient anatomy based on the surgical instrument's orientation and optical field of view; and a combining device to inject the three dimensional perspective volumetric image of the preexisting image data in the optical field of view of the surgical instrument. 