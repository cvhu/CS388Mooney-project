A robotic system and a related method for reproducing a real person's facial expression and speech simultaneously and synchronously is provided herein. The robotic system comprises at least a robotic head which in turn comprises a speaker, a plurality of face actuators, and a computing engine. The robotic head drives the speaker and the face actuators synchronously based on a speech segment and a sequence of time-stamped control vectors so that the robotic system could mimic a real person's facial expression and speech. The speech segment and the sequence of time-stamped control vectors are retrieved from a storage device of the robotic system, or from an external source via an appropriate communication mechanism.
Claims What is claimed is: 1. A robotic system for reproducing a person's facial expression and speech comprising: a robotic head comprising a speaker, a plurality of face actuators, and a computing engine connected to said speaker and said face actuators, said computing engine driving said face actuators based on a sequence of sets of control parameters while delivering speech from said speaker based on a speech segment; wherein each of said sets of control parameters includes a time stamp and a control parameter corresponding to each of said face actuators; and said computing engine drives said face actuators at appropriate time specified by the time stamps of said sets of control parameters so that said face actuators are activated according to the corresponding control parameters to reproduce said person's facial expression. 2. The robotic system according to claim 1, wherein said computing engine comprises a storage device; and said computing engine retrieves said speech segment and said sequence of sets of control parameters from said storage device. 3. The robotic system according to claim 1, wherein said computing engine comprises an appropriate interface; and said computing engine retrieves said speech segment and said sequence of sets of control parameters from an external device via said interface. 4. The robotic system according to claim 1, wherein said speech segment is a segment of pre-recorded speech in an appropriate format. 5. The robotic system according to claim 1, wherein said speech segment is a sequence of synthesis commands; each of said synthesis commands is associated with a time stamp; said computing engine artificially synthesizes speech at the time specified by the time stamp of a synthesis command according to said synthesis command. 6. The robotic system according to claim 1, further comprising: an interpretation device, said interpretation device generating said speech segment from a recorded speech; said interpretation device generating said sequence of sets of control parameters from a recorded video; and said interpretation device delivering said speech segment and said sequence of sets of control parameters to said computing engine via an appropriate mechanism. 7. The robotic system according to claim 6, wherein said appropriate mechanism is one of the following two mechanisms: a removable media and a direct link. 8. The robotic system according to claim 7, wherein said direct link is a local area network. 9. The robotic system according to claim 6, wherein said interpretation device allows a user to add an additional set of control parameters into said sequence of sets of control parameters, and to delete and modify an existing set of control parameters from said sequence of sets of control parameters. 10. The robotic system according to claim 6, wherein said recorded speech and said recorded video are combined in a single recording. 11. The robotic system according to claim 6, wherein said interpretation device allows a user to align said recorded speech and said recorded video so that said recorded speech and said recorded video have synchronized staffing time. 12. The robotic system according to claim 6, wherein said interpretation device comprises at least one of a camera and a microphone; said interpretation device operates at least one of said camera and said microphone to capture said person's performance over a period of time and to produce at least one of said recorded speech and said recorded video. 13. The robotic system according to claim 6, wherein said interpretation device obtains at least one of said recorded speech and said recorded video from an external source via an appropriate mechanism. 14. The robotic system according to claim 13, wherein said appropriate mechanism is one of the following two mechanisms: a removable media and a direct link. 15. The robotic system according to claim 14, wherein said direct link is a local area network. 16. The robotic system according to claim 6, further comprising: a recording device comprising at least one of a camera and a microphone; said recording device operating at least one of said camera and said microphone to capture said person's performance over a period of time to produce at least one of said recorded speech and said recorded video, and said recording device delivering at least one of said recorded speech and said recorded video to said interpretation device via an appropriate mechanism. 17. The robotic system according to claim 16, wherein said appropriate mechanism is one of the following two mechanisms: a removable media and a direct link. 18. The robotic system according to claim 17, wherein said direct link is a local area network. 19. The robotic system according to claim 16, wherein said recorded speech and said recorded video are combined in a single recording. 20. A method for reproducing a person's facial expression and speech on a robotic head, said robotic head comprising a speaker and a plurality of face actuators, said method comprising the steps of: (1) operating at least a camera and a microphone to capture said person's performance over a period of time into a recorded speech and a recorded video having synchronized timing information for said person's facial expression and speech; (2) processing said recorded speech and said recorded video to produce a speech segment and a sequence of sets of control parameters, each of said sets of control parameters including a time stamp derived from said timing information and a control parameter corresponding to each of said face actuators, each of said control parameters controlling one of said face actuators respectively; and (3) delivering said person's speech based on said speech segment via said speaker while driving said face actuators at appropriate time specified by the time stamps of said sets of control parameters so that said face actuators are activated according to the corresponding control parameters to reproduce said person's facial expression. 21. The method according to claim 20, further comprising the following step between said step (2) and said step (3): (2.5) allowing a user to add an additional set of control parameters into said sequence of sets of control parameters, and to delete and modify an existing set of control parameters from said sequence of sets of control parameters. 22. The method according to claim 20, wherein said speech segment is obtained by converting said recorded speech into an appropriate format. 23. The method according to claim 20, wherein said speech segment is a sequence of synthesis commands derived from said recorded speech; each of said synthesis commands is associated with a time stamp; and delivering said person's speech is by artificially synthesizing speech at the time specified by the time stamps of said synthesis commands according to said synthesis commands. 24. The method according to claim 20, wherein said recorded speech and said recorded video are combined in a single recording. 25. The method according to claim 20, further comprising the following step between said step (1) and said step (2): (1.5) allowing a user to align said recorded speech and said recorded video so that said recorded speech and said recorded video have synchronized start time. 26. A method for reproducing a person's facial expression and speech on a robotic head, said robotic head comprising a speaker and a plurality of face actuators, said method comprising the steps of: (1) obtaining a recorded speech and a recorded video of said person's speech and facial expression, said recorded speech and said recorded video having appropriate timing information; (2) processing said recorded speech and said recorded video to produce a speech segment and a sequence of sets of control parameters, each of said sets of control parameters including a time stamp derived from said timing information and a control parameter corresponding to each of said face actuators, each of said control parameters controlling one of said face actuators respectively; and (3) delivering said person's speech based on said speech segment via said speaker while driving said face actuators at appropriate time specified by the time stamps of said sets of control parameters so that said face actuators are activated according to the corresponding control parameters to reproduce said person's facial expression. 27. The method according to claim 26, further comprising the following step between said step (2) and said step (3): (2.5) allowing a user to add an additional set of control parameters into said sequence of sets of control parameters, and to delete and modify an existing set of control parameters from said sequence of sets of control parameters. 28. The method according to claim 26, wherein said speech segment is obtained by converting said recorded speech into an appropriate format. 29. The method according to claim 26, wherein said speech segment is a sequence of synthesis commands derived from said recorded speech; each of said synthesis commands is associated with a time stamp; and delivering said person's speech is by artificially synthesizing speech at the time specified by the time stamps of said synthesis commands according to said synthesis commands. 30. The method according to claim 26, wherein said recorded speech and said recorded video are combined in a single recording. 31. The method according to claim 26, further comprising the following step between said step (1) and said step (2): (1.5) allowing a user to align said recorded speech and said recorded video so that said recorded speech and said recorded video have synchronized start time. 