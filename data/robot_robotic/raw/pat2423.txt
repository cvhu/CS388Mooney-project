A method and system determines a pose of an object by comparing an input range image acquired of a scene including the input object to each of a set of reference range image of a reference object, such that each reference range images has an associated different pose, and the reference object is similar to the input object. Then, the pose associated with the reference range image which best matches the input range image is selected as the pose of the input object.
Claims We claim: 1. A method for determining a pose of an object, comprising the steps of: comparing an input range image acquired of a scene including an input object to each of a plurality of reference range image of a reference object, such that each reference range images has an associated different pose, and the reference object is similar to the input object, wherein the comparing is performed on a per pixel basis; and selecting the associated pose of the reference range image which best matches the input range image as the pose of the input object in the scene, and wherein the comparing between the reference range image R and the input range image I uses an error function: .epsilon..function..times..times..times..epsilon..function..lamda..times.- .times..times..times..epsilon..function. ##EQU00005## where a cover error is .epsilon..sub.cover(u, v, x, y), a range error is .epsilon..sub.range(u, v, x, y, z), (u, v) are coordinated of pixels in the input range image, (x, y, z) are coordinates of pixels in the set the reference range images, .lamda. a weight, and N .sub.cover and N.sub.range are normalization factors. wherein the steps are performed in a graphics processor unit. 2. The method of claim 1, further comprising: scanning the reference object to construct a 3D model of the reference object; and rendering the 3D model for the plurality of different poses to generate the set of reference range images. 3. The method of claim 1, further comprising: scanning the scene to acquire a scan of the input object, the scan having range values; smoothing the scan to produce a smoothed scan; detecting edges in the smoothed scan to produce a distance field input image; and combining the distance field input image with the range values of the scan to generate the input range image. 4. The method of claim of claim 1, in which a best matching reference range image is selected using a downhill simplex procedure. 5. The method of claim 1, in which the cover error of the pixel (u, v) of the input range image and a pixel (x, y, z) in the reference range image for a Euclidean distance transform (EDT) is .epsilon..function..function..function..times..times..function..gtoreq..t- imes..times. ##EQU00006## 6. The method of claim 1, in which the cover error is minimal when silhouettes of the input object in the input range image and the reference object in the reference image match. 7. The method of claim 1, in which the cover error normalization factor is: N.sub.cover=|{(u, v)EDT.sub.R(u+x, v+y).gtoreq.0}|. 8. The method of claim 1, in which range error is: .epsilon..function..function..function..times..times..function..gtoreq..t- imes..times..function..gtoreq..times..times. ##EQU00007## 9. The method of claim 8, in which the range error values of all pixels in the reference range images I and the input range image R that overlap. 10. The method of claim 1, in which the range error normalization factor is: N.sub.depth=|{(u, v)|EDT.sub.I(u, v).gtoreq.0.LAMBDA.EDT.sub.R(u+x, v+y).gtoreq.0}|. 11. The method of claim 1, in which the best match minimizes the error function .theta..PHI..sigma..times..times..theta..PHI..sigma..times..time- s..epsilon..function..theta..PHI..sigma..times. .times..times. .times..times. ##EQU00008## where R is the reference range image of the 3D model rendered with rotation angles(.theta., .phi., .sigma.), and step 1 determines an error between each reference range image and the input range image using a downhill simplex method for the translation values (x, y, z), and step 2 selects the associated pose of the reference range image R with a lowest global error. 12. The method of claim 11, in which parameters of the error function are minimized by locating pixels in the reference range images R and the input range image I, that corresponds to a center of gravity of the reference object and the input object, respectively. 13. The method of claim 1, in which the scene includes multiple objects, and further comprising: selecting a nearest object in the scene as the input object. 14. The method of claim 1, in which a Euclidian distance transform is applied to each input range image. 15. The method of claim 1, in which the comparing step is performed in a graphic processor unit. 16. The method of claim 1, in which multiple reference range images are compared in parallel with the input range image in a plurality of shader processors of a graphic processing unit. 