In a computer system functioning as an autonomous agent with a robotic controller wherein the computer system need not be programmed to perform a specific task, values in an array of error variables are obtained from comparison between an array of resources variables used to indicate resource use and state of an Autonomous Agent and an array of corresponding desired levels for the resource variables. Each individual value in the array of error variables represents an error in the actual value of a particular resource variable as compared to its desired value. Derivative variables for the resource variables and for parameters controlling behavior of the Autonomous Agent are created. The values of the array of error variables and their associated derivative variables are used to adjust values of the behavior-controlling parameters thereby modifying behavior of the Autonomous Agent.
Claims What is claimed is: 1. In an autonomous agent, a method for controlling self learning behavior of said autonomous agent comprising: using resource variables for monitoring level of use of internal resources and condition of said autonomous agent; generating error variables of said resource variables, said error variables representing differences in value of desired values of said resource variables and actual values of said resource variables; generating resource derivative variables of said resource variables, each of said resource derivative variables representing the derivative of a single resource variable with a single parameter in said autonomous agent, wherein each said single parameter used in said resource derivative variables controls behavior of said autonomous agent, and using said error variables and said resource derivative variables to adjust parameters in said autonomous agent to control behavior of said autonomous agent. 2. The method of claim 1 wherein the step of generating said resource derivative variables includes using a State Machine Block, said State Machine Block being trained to model sources of said resource variables. 3. The method of claim 2 wherein said State Machine Block is trained by the use of error variables resulting from a comparison of outputs of said State Machine Block with outputs of said source of resource variables, and by use of derivative variables of outputs of said State Machine Block with parameters in said State Machine Block. 4. The method of claim 1 wherein the step of using said error variables and said resource derivative variables involves use of change variables, wherein a change variable is provided for each said resource variable and the value of said change variable is calculated using: ##EQU18## where: change.sub.i.vertline..sub.n is the value of the change variable for the y.sub.i resource variable at the n.sup.th data point; ##EQU19## is the n.sup.th data point value of the resource derivative variable for resource variable y.sub.i and parameter a.sub.j ; .DELTA.a.sub.j the j.sup.th value of the array of parameter change values; wherein said parameter change values are adjusted according to the equation: ##EQU20## where: Training Error.sub.i.vertline..sub.n is n.sup.th data point value of the training error, the value of said training error being determined from: Training Error.sub.i.vertline..sub.n =error.sub.i.vertline..sub.n -change.sub.i.vertline..sub.n ; the value of Beta.sub.i.vertline..sub.n is calculated using: ##EQU21## K is a value very close to but less than one. 5. The method of claim 4 wherein K is a variable having a different value for each resource variable being trained. 